import tkinter as tk
from PIL import Image, ImageTk
import time
import threading
import pyttsx3
from transformers import pipeline
import os
import speech_recognition as sr
from deep_translator import GoogleTranslator

# åˆå§‹åŒ–æ¨¡å‹ & TTS
emotion_classifier = pipeline(
    "text-classification",
    model="j-hartmann/emotion-english-distilroberta-base",
    return_all_scores=False
)
engine = pyttsx3.init()

# Tkinter åˆå§‹åŒ–
root = tk.Tk()
root.title("AI Vtuber")
canvas = tk.Canvas(root, width=512, height=512)
canvas.pack()

# é¡¯ç¤ºç›®å‰è¡¨æƒ…çš„ Label
emotion_label = tk.Label(root, text="ç›®å‰è¡¨æƒ…ï¼šå°šæœªè¾¨è­˜", font=('Arial', 14))
emotion_label.pack(pady=5)

# åœ–ç‰‡å¿«å–
images = {}
IMAGE_DIR = "face"

def load_image(filename):
    path = os.path.join(IMAGE_DIR, filename)
    if path not in images:
        img = Image.open(path).resize((512, 512))
        images[path] = ImageTk.PhotoImage(img)
    return images[path]

# é è¨­åœ–å±¤ ID
face_id = None
mouth_id = None

# å˜´å·´å‹•ç•«å‡½å¼
def speak_and_animate(text):
    global mouth_id
    duration = max(len(text) / 10, 2)
    end_time = time.time() + duration
    while time.time() < end_time:
        canvas.itemconfig(mouth_id, image=load_image("mouth_open.png"))
        root.update()
        time.sleep(0.15)
        canvas.itemconfig(mouth_id, image=load_image("mouth_closed.png"))
        root.update()
        time.sleep(0.15)
    canvas.delete(mouth_id)

# TTS æ’­æ”¾å‡½å¼
def engine_runner(text):
    engine.say(text)
    engine.runAndWait()

# ä¸»é‚è¼¯ï¼šè¼¸å…¥æ–‡å­— â†’ è¡¨æƒ…åˆ¤æ–· + å˜´å·´å‹•ç•«
def vtuber_say(text):
    global face_id, mouth_id

    # ä¸­æ–‡è½‰è‹±æ–‡ï¼ˆç¿»è­¯å¤±æ•—å‰‡ä¿ç•™åŸæ–‡ï¼‰
    try:
        english_text = GoogleTranslator(source='zh-TW', target='en').translate(text)
    except Exception as e:
        print("âš ï¸ ç¿»è­¯å¤±æ•—ï¼Œä½¿ç”¨åŸæ–‡ï¼š", e)
        english_text = text

    # æƒ…ç·’åˆ†æ
    result = emotion_classifier(english_text)[0]
    label = result["label"]

    # é¡¯ç¤ºæƒ…ç·’åç¨±
    emotion_label.config(text=f"ç›®å‰è¡¨æƒ…ï¼š{label}")

    # è¡¨æƒ…åœ–ç‰‡å°æ‡‰
    if label == "joy":
        face_img = "happy.png"
    elif label == "sadness":
        face_img = "sad.png"
    elif label == "anger":
        face_img = "angry.png"
    elif label == "surprise":
        face_img = "surprise.png"
    else:
        face_img = "neutral.png"

    # æ¸…é™¤å‰ä¸€å¼µè‡‰èˆ‡å˜´å·´
    if face_id:
        canvas.delete(face_id)
    if mouth_id:
        canvas.delete(mouth_id)

    # é¡¯ç¤ºæ–°è‡‰èˆ‡å˜´å·´ï¼ˆåˆå§‹é–‰å£ï¼‰
    face_id = canvas.create_image(0, 0, anchor=tk.NW, image=load_image(face_img))
    mouth_id = canvas.create_image(0, 0, anchor=tk.NW, image=load_image("mouth_closed.png"))

    # åŒæ™‚å•Ÿå‹•è²éŸ³èˆ‡å˜´å·´å‹•ç•«
    threading.Thread(target=engine_runner, args=(text,)).start()
    threading.Thread(target=speak_and_animate, args=(text,)).start()

# éµç›¤è¼¸å…¥æ¨¡å¼
def run_vtuber():
    text = entry.get()
    vtuber_say(text)

# éŒ„éŸ³ + èªéŸ³è¾¨è­˜
def record_and_recognize():
    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    print("ğŸ¤ é–‹å§‹éŒ„éŸ³...")
    with mic as source:
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)

    try:
        print("ğŸ§  è¾¨è­˜ä¸­...")
        text = recognizer.recognize_google(audio, language='zh-TW')
        print("âœ… ä½ èªªäº†ï¼š", text)
        entry.delete(0, tk.END)
        entry.insert(0, text)
        vtuber_say(text)
    except sr.UnknownValueError:
        print("ğŸ˜… è½ä¸æ¸…æ¥š...")
    except sr.RequestError as e:
        print("âŒ Google è¾¨è­˜å‡ºéŒ¯ï¼š", e)

# ä»‹é¢å…ƒä»¶
entry = tk.Entry(root, width=40, font=('Arial', 16))
entry.pack(pady=10)

btn_text = tk.Button(root, text="ç”¨æ–‡å­—è®“ Vtuber èªªè©±", command=run_vtuber, font=('Arial', 14))
btn_text.pack(pady=5)

btn_voice = tk.Button(root, text="ğŸ™ï¸ éŒ„éŸ³è®“ Vtuber æ¨¡ä»¿ä½ ", command=lambda: threading.Thread(target=record_and_recognize).start(), font=('Arial', 14))
btn_voice.pack(pady=5)

root.mainloop()
